<!doctype html>
<html>
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Tyler Ransom by linjucs</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Ju Lin</h1>
        <p>PhD Candidate<br>Clemson University</p>
        <p><a href="https://cecas.clemson.edu/fctl/">Future Computing Technologies Lab</a></br>ECE department At Clemson University</p>
    <h3><p class="view"><a href="https://linjucs.github.io/">Home</a></p></h3>
        <h3><p class="view"><a href="https://linjucs.github.io/research.html">Research</a></p></h3>
    <h3><p class="view"><a href="https://linjucs.github.io/research/CV.pdf">CV</a></p></h3>  
        <h3><p class="view"><a href="https://linjucs.github.io/code.html">Code</a></p></h3> 
        <h3><p class="view"><a href="https://linjucs.github.io/teaching.html">Teaching</a></p></h3> 
        <h3><p class="view"><a href="https://linjucs.github.io/personal.html">Personal</a></p></h3>
    <p class="view"><b>Social</b><br>
        <a href="mailto:jul@clemson.edu" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
        <a href="https://scholar.google.com/citations?user=Zb5SLNIAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
        <a href="https://orcid.org/0000-0002-6910-0363"><i class="ai ai-fw ai-orcid-square"></i> ORCID</a><br>
        <a href="http://ideas.repec.org/f/pra541.html"><i class="fa fa-fw fa-share-alt-square"></i> RePEc</a><br>
        <a href="http://github.com/linjucs"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
        <a href="http://twitter.com/linjucs" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br>
        <a href="http://linkedin.com/in/linjucs" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a><br>

    <p><b>Contact:</b><br>Department of Economics<br>University of Oklahoma<br>322 CCD1, 308 Cate Center Drive<br>Norman, OK 73072</p>
      </header>
      <section>

    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Journal Papers</h2>
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://link.springer.com/article/10.1007/s11265-018-1334-2">Improving Mandarin Tone Recognition Based on DNN by Combining Acoustic and Articulatory Features Using Extended Recognition Networks</a>  (with <a>Wei Li, Yingming Gao, Yanlu Xie, Nancy F. ChenSabato Marco Siniscalchi, Jinsong Zhang and Chin-hui Lee</a>) <br> <i>Journal of Signal Processing Systems, July 2018, Volume 90, Issue 7, pp 1077-1087</i> <br><button class="accordion">   
   Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">In this paper, we investigate the effectiveness of articulatory information for Mandarin tone modeling and recognition in a deep neural network â€“ hidden Markov model (DNN-HMM) framework. In conventional approaches, prosodic evidence (e.g., F0, duration and energy) is used to build tone classifiers, we here propose performance enhancement techniques in three areas: (i) adding articulatory features (AFs) and acoustic features, such as MFCCs (Mel frequency cepstrum coefficients), for tone modeling; (ii) adopting phone-dependent tone modeling; and (iii) using tone-based extended recognition network (ERN) to reduce the tone search space. The first approach is feature-related, it explicitly employs the AFs as a form of tonal features and is implemented through a multi-stage procedure. The second approach is model-related and directly extends to phone-dependent tone modeling so that each modeling unit (e.g., tonal phone) not only contains tone information, but also integrates the phone/articulatory information. Finally, the third technique is search-related with a phone-dependent tone-based expanding searching network. A series of comprehensive experiments is conducted using different input feature sets. It is demonstrated that (i) tone recognition accuracy is boosted by incorporating articulatory information, and (ii) ERN, attains the lowest tone error rate of 7.17%, with a 56% relative error reduction from the prosody-only baseline system error of 16.36%. </div></p>
<p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://linjucs.github.io/research/prosody2016.pdf">Automatic Mandarin Prosody Boundary Detection Based on Tone Nucleus and DNN Model</a>  (with <a>Wei Zhang, Yanlu Xie, Jinsong Zhang</a>) <br> <i>Journal of Chinese Information Processing, 2016, Volume 30, Issue 6, pp 35-39</i> <br><button class="accordion">    
   Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">Prosody boundary plays an important role in naturalness and intelligibility of verbal expressions. Thus, prosody modeling is also an important aspect of speech synthesis and understanding. Focused on the interaction of adjacent tones, we propose a method of prosody boundary detection based on tone nucleus and DNN model. This method calculates the boundary-related parameters by applying the tone nucleus features. Then, the parameters are modeled by the deep neural network. For comparison, the baseline system chooses syllable the acoustic feature. The experimental results show a relative 4% improvement achieved by the proposed method. </div></p>   


<p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://linjucs.github.io/research/creakyvoice2015.pdf"> The Effect of Age and Gender on Creaky Voice in Tone3 of Beijing Dialect</a>  (with <a>Zhijing Liu, Yanlu Xie, Jinsong Zhang</a>) <br> <i>Journal of Chinese Phonetics, 2015</i> <br><button class="accordion">       
   Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">The correlation between acoustic features and social identity has long been studied in the area of sociolinguistics. This research focuses on how factors like age and gender impact the distribution of creaky voice in Tone3 of Beijing dialect. </div></p> 

<hr>
    <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conference Papers</h2>
<p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.researchgate.net/profile/Wei_Zhang384/publication/329519407_A_Multi-modal_Soft_Targets_Approach_for_Pronunciation_Erroneous_Tendency_Detection/links/5c0ccb9e299bf139c749ad55/A-Multi-modal-Soft-Targets-Approach-for-Pronunciation-Erroneous-Tendency-Detection.pdf">A Multi-modal Soft Targets Approach for Pronunciation Erroneous Tendency Detection</a>  (with <a>Wei Zhang, Linxuan Wei, Yanlu Xie, Jinsong Zhang</a>) <br> <i>ISCSLP 2018</i> <br><button class="accordion">
   Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">Detecting pronunciation erroneous tendency (PET) can provide detailed instructive feedback for second language learners in computer aided pronunciation training (CAPT). In this paper, we utilize soft targets with knowledge from various models for improving the detection performance of PET. First, we examined the effectiveness of soft targets in three single systems by replacing hard targets with soft targets directly for mispronunciation detection. Then, two kinds of methods using multi-modal soft targets are proposed in this paper: 1) explicit combination, which uses multi-modal soft targets as the final targets by weighted linear combination; 2) implicit combination, which employs the multi-task framework to combine soft targets. Experimental results showed that the detection performance of PET can be improved by using both single soft targets and multi-modal soft targets. Moreover, using multi-modal soft targets within multi-task framework achieve the best results in pronunciation error detection task, and it is more efficient than conventional ensemble methods which require multiple decoding runs or forward passes. </div></p>
   

<p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://pdfs.semanticscholar.org/8fa6/300a035a0cd97e612d1c941572758dc74464.pdf">Automatic Pronunciation Evaluation of Non-Native Mandarin Tone by Using Multi-Level Confidence Measures.</a>  (with <a>Yanlu Xie, Jinsong Zhang</a>) <br> <i>INTERSPEECH 2016</i> <br><button class="accordion">
   Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">Automatic evaluation of tonal production plays an important role in a tonal language Computer-Assisted Pronunciation Training (CAPT) system. In this paper, we propose an automatic evaluation method for non-native Mandarin tones. The method applied multi-level confidence measures generated from Deep Neural Network (DNN). The confidence measures consisted of Log Posterior Ratios (LPR), Average Frame-level Log Posteriors (AFLP) and Segment-level Log Posteriors (SLP). The LPR was calculated between the correct tone model and competing tone models. The AFLP and LPR were obtained from frame-level scores. And the SLP was directly derived from segment-level scores. The multi-level confidence measures were modeled with a support vector machine (SVM) classifier. For comparison, three experiments were conducted according to different features: AFLP+LPR, SLP only and AFLP+LPR+SLP. The experimental results showed that the performance of the system which used multi- level confidence measures was the best, achieving a FRR of 5.63% and a DA of 82.45%, which demonstrated the efficiency of the proposed method.</div></p>
 <hr>

      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
